{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22262cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd()\n",
    "os.chdir('C://kiosks_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4237d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e79485",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('main\\\\components\\\\menu_maneger\\\\pdf_dir\\\\reviewed_menu_data.json', 'r') as menu_json_file:\n",
    "    data = json.load(menu_json_file)\n",
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118853c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_name': 'Dhaniya Paneer Tikka',\n",
       "  'price': '675',\n",
       "  'description': 'Apricot, almond slivers, corn chat, pickled mango dip'},\n",
       " {'item_name': 'Chicken or Paneer Kathi Roll',\n",
       "  'price': '750',\n",
       "  'description': 'Grilled chicken tikka or paneer tikka, green apple relish, mint chutney'},\n",
       " {'item_name': 'Pav Bhaji',\n",
       "  'price': '685',\n",
       "  'description': 'Classic Bombay street food, spicy mashed potatoes & vegetable mixture,'},\n",
       " {'item_name': 'Kheema Ghotala',\n",
       "  'price': '785',\n",
       "  'description': 'Bombay street food classic, spiced minced lamb, egg,'},\n",
       " {'item_name': 'Goan Pomfret Curry',\n",
       "  'price': '1325',\n",
       "  'description': 'Mildly spiced coconut curry flavoured with Garcinia indica rind,'},\n",
       " {'item_name': 'Vegetable Club Sandwich',\n",
       "  'price': '685',\n",
       "  'description': 'Russian salad, tomato, cheese, iceberg lettuce'},\n",
       " {'item_name': 'Vegetarian Panini sandwich',\n",
       "  'price': '685',\n",
       "  'description': 'Herbed mushroom, grilled pimentos, avocado, provolone cheese,'},\n",
       " {'item_name': 'Classic Club Sandwich',\n",
       "  'price': '685',\n",
       "  'description': 'Chicken slices, fried egg, ham, cheese, tomato, iceberg lettuce'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c2161d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = 'get me couple of Classic Club Sandwich along with two Kheema Ghotala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "598bfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BM9rYrRmP1hyZCLGTGzb9PaoZajbH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\\n[\\n    {\\n        \"item_name\": \"Classic Club Sandwich\",\\n        \"quantity\": 2\\n    },\\n    {\\n        \"item_name\": \"Kheema Ghotala\",\\n        \"quantity\": 2\\n    }\\n]\\n```', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1744620828, model='gpt-4-1106-preview', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=52, prompt_tokens=351, total_tokens=403, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    # messages=[\n",
    "    #     {\"role\": \"system\", \"content\": \"You're a restaurant order extractor. Match spoken order to menu items.\"},\n",
    "    #     {\"role\": \"user\", \"content\": f\"Menu items: {data}\\nCustomer said: {sample_input}\\n\\nReturn a JSON list with item_name and quantity.\"}\n",
    "    # ],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You're a restaurant order extractor. Match spoken order to menu items and return a JSON list of item_name and quantity.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Menu items: {data}\\nCustomer said: {sample_input}\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    # # functions=[\n",
    "    #     {\n",
    "    #         \"name\": \"extract_order\",\n",
    "    #         \"parameters\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"order\": {\n",
    "    #                     \"type\": \"array\",\n",
    "    #                     \"items\": {\n",
    "    #                         \"type\": \"object\",\n",
    "    #                         \"properties\": {\n",
    "    #                             \"item_name\": {\"type\": \"string\"},\n",
    "    #                             \"quantity\": {\"type\": \"integer\"}\n",
    "    #                         },\n",
    "    #                         \"required\": [\"item_name\", \"quantity\"]\n",
    "    #                     }\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"order\"]\n",
    "    #         }\n",
    "    #     }\n",
    "    # ],\n",
    "    # function_call={\"name\": \"extract_order\"}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5162266",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call = response.choices[0].message.content\n",
    "# function_call = response.choices[0].message.function_call\n",
    "# order_data = json.loads(function_call)\n",
    "# order_data\n",
    "# Remove markdown code block syntax\n",
    "json_str = function_call.split('```json\\n')[1].split('\\n```')[0]\n",
    "order_items = json.loads(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6706a22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_name': 'Classic Club Sandwich', 'quantity': 2},\n",
       " {'item_name': 'Kheema Ghotala', 'quantity': 2}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe950ac",
   "metadata": {},
   "source": [
    "## Trying from free api model GROQ or Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a7de0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "071b2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You're a restaurant order extractor. Match spoken order to menu items with price and return a just JSON list of item_name, price and price quantity. Nothing other than that\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Menu items: {data}\\nCustomer said: {sample_input}\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    # top_p=1,\n",
    "    # stream=True,\n",
    "    # stop=None,\n",
    ")\n",
    "\n",
    "# for chunk in completion:\n",
    "#     print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61e47be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'item_name': 'Classic Club Sandwich', 'price': 685, 'quantity': 2}, {'item_name': 'Kheema Ghotala', 'price': 785, 'quantity': 2}]\n"
     ]
    }
   ],
   "source": [
    "output_json = completion.choices[0].message.content\n",
    "order_items = json.loads(output_json)\n",
    "print(order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d691bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C://Kiosks_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f6fb1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "path = 'models\\\\m'\n",
    "import os\n",
    "if not os.listdir(path):\n",
    "    print('empty') \n",
    "else:\n",
    "    print('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12208e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(self):\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config['base_model_id'], token=token)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config['base_model_id'],\n",
    "            device_map=\"auto\",            # Automatically uses GPU if available\n",
    "            torch_dtype=\"auto\",           # Adjusts precision\n",
    "            trust_remote_code=True,\n",
    "            token= token\n",
    "        )\n",
    "\n",
    "        # Optional: wrap in a pipeline\n",
    "        llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "        \n",
    "        # Prompt (e.g., for restaurant order understanding)\n",
    "                \n",
    "        prompt =[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You're a restaurant order extractor. Match spoken order to menu items with price and return a just JSON list of item_name, price and price quantity. Nothing other than that\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Menu items: {data}\\nCustomer said: {sample_input}\"\n",
    "                }]\n",
    "        prompt = json.dumps(prompt)\n",
    "                \n",
    "        response = llm(prompt, max_new_tokens=256, do_sample=False)[0][\"generated_text\"]\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dfed7",
   "metadata": {},
   "source": [
    "# Loading and Testing Fine tunned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c9e415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Kiosks_'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C://Kiosks_')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown quantization type, got bitsandbytes - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'quark', 'eetq', 'higgs', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao', 'bitnet', 'vptq', 'spqr', 'fp8']. Hence, we will skip the quantization. To remove the warning, you can delete the quantization_config attribute in config.json\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1310720, 1]) from checkpoint, the shape in current model is torch.Size([1024, 2560]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m config = AutoConfig.from_pretrained(model_path, local_files_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Optional: modify config here if needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m model = model.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:824\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    822\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[43m_load_parameter_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     hf_quantizer.create_quantized_param(\n\u001b[32m    828\u001b[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[32m    829\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\transformers\\modeling_utils.py:712\u001b[39m, in \u001b[36m_load_parameter_into_model\u001b[39m\u001b[34m(model, param_name, tensor)\u001b[39m\n\u001b[32m    710\u001b[39m module, param_type = get_module_from_name(model, param_name)\n\u001b[32m    711\u001b[39m \u001b[38;5;66;03m# This will check potential shape mismatch if skipped before\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1310720, 1]) from checkpoint, the shape in current model is torch.Size([1024, 2560])."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Use raw string or escape backslashes correctly\n",
    "model_path = r\"C:\\\\kiosks_\\\\models\\\\gemma3_4b_32bit\"\n",
    "\n",
    "# Load tokenizer from local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# Load model in float32 for CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32,\n",
    "    local_files_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473f980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
