from main.components.menu_maneger.config.menu_maneger_config import pdf_source
import re, os, json
from main.functions.common_function import save_json_file
import pdfplumber
from typing import List, Dict, Any
from dotenv import load_dotenv 
import groq
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document   
import pytesseract
from transformers import LayoutLMv3Processor
from PIL import Image

load_dotenv()
groq_api_key = os.getenv('GROQ_API_KEY')
groq_client = groq.Client(api_key=groq_api_key)


class pdf_extractor:
    def __init__(self):
        obj = pdf_source()
        self.source, self.prompt = obj.create_source_dir()
        self.pdf_dir = self.source['pdf_dir']
        self.pdf_url = self.source['pdf_url']
        self.extracted_menu_dir = self.source['extracted_menu']
        self.final_json_name = self.source['final_json_name']
        self.extracter_model_name = self.source['extracter_model_name']

        self.embeddings = HuggingFaceEmbeddings(
           model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={'device': 'cpu'}
        )



    def manual_extract_pdf_text(self)-> dict:
        """
          Function to extract text from a pdf file
          Args:
                str (pdf): pdf file path
          Returns: josn file with extracted text
         """
        raw_menu_items = []
        # categories = []
        with pdfplumber.open(self.pdf_url) as pdf_filess:
            for pages in pdf_filess.pages:
                # pages = pdf_filess.pages[3]
                text = pages.extract_text()
                text_new_lines = text.split('\n')
                for line in text_new_lines:
                    match = re.search(r'^(.+?)\s+(\d{2,5}(?:\.\d{2})?)$', line.strip())
                    if match:
                        item_name = match.group(1).strip()
                        price = match.group(2).strip()
                        description_line = text_new_lines[text_new_lines.index(line) + 1]
                        if not description_line:
                            description_line = ""
                            if i + 1 < len(line):
                                next_line = line[i + 1].strip()
                                next_match = re.match(r'^(.+?)\s+(\d{2,5}(?:\.\d{2})?)$', next_line)
                                if not next_match:
                                    description_line = next_line
                                    i += 1  # skip the description line in next iteration
        
            
                        raw_menu_items.append({
                            'item_name': item_name,
                            'price': price,
                            'description': description_line.strip()
                        })
   
        output_file_path = os.path.join(self.extracted_menu_dir, "raw_manual_menu_data.json")
        save_json_file(path=output_file_path, data=raw_menu_items)

        return output_file_path
    

    def human_in_loop_manual_extractor(self, raw_menu_items: json, file_name:str = 'api_menu_extractir.json'):
        '''
        Function to review the extracted menu items and allow for manual editing or deletion.
        Args: 
            raw_menu_items (json): json file with extracted menu items
        Returns:
            reviewed_data (json): json file with reviewed and edited menu items
        '''
        
        print('Review the JSON data before finalizing')

        reviewed_data = []
        with open(raw_menu_items , 'r') as f:
            raw_menu_items = json.load(f)
    
        for idx, items in enumerate(raw_menu_items):
            print('---------------------------------')
            print(f"[{idx+1}] Item Name      : {items['item_name']}")
            print(f"    Price          : {items['price']}")
            print(f"    Description    : {items['description']}")
            print('---------------------------------')
        
            action = input("Is this correct? (y/n): or press 'd' to delete or 'all' to save raw json file   âž¤ ").strip().lower()
        
            if action == 'y':
                print("âœ… Item confirmed.")
                reviewed_data.append(items)
        
            elif action == 'n':
                print("âœï¸  Editing the item.")
                items['item_name'] = input(f"    Edit Item Name [{items['item_name']}]: ") or items['item_name']
                items['price'] = input(f"    Edit Price [{items['price']}]: ") or items['price']
                items['description'] = input(f"    Edit Description [{items['description']}]: ") or items['description']
                reviewed_data.append(items)
        
            elif action == 'd':
                print("ðŸ—‘ï¸  Item deleted.")
                # Do not add to reviewed_data

            elif action == 'all':
                break

            else:
                print("âš ï¸  Invalid input. Please enter 'y', 'n', or 'd'")
                # Optionally re-prompt here if you want strict control
        
        output_file_path = os.path.join(self.extracted_menu_dir, f"manual_{self.final_json_name}")
        if not reviewed_data:
            print("âŒ No items were confirmed. Exiting...")
            save_json_file(path=output_file_path, data= raw_menu_items)

            return raw_menu_items
        
        save_json_file(path=output_file_path, data= reviewed_data)
        return reviewed_data
    
    
# code for extracting the menu using the LLM (API)
# Need to call seperate Human-In_loop function for this abpve LLM function because of it handle double for loop (ie categories)ðŸ‘‡ðŸ‘‡ðŸ‘‡ðŸ¤–ðŸ¤–ðŸ¤–



    def load_and_process_docs(self, file_) -> List[Document]:
        '''
        Load document from the specified directory

        split document into chuck
        Args:
            file (str) : file Path
            pdf_maneger:
                    pdf_path: source file
        '''
        all_docs = []
        if file_.endswith('.pdf'):
            loader = PyPDFLoader(file_)
            documnets = loader.load()
            all_docs.extend(documnets)
            document = all_docs
        
        elif file_.endswith('.txt'):
            loader = TextLoader(file_)
            documnets = loader.load()
            all_docs.extend(documnets)
            document = all_docs           
        
        text_spliter = RecursiveCharacterTextSplitter(
            chunk_size = 512,
            chunk_overlap = 50
        )
        chunk = text_spliter.split_documents(document)
        print(f'Split into {len(chunk)}')
        self.vectorstore = FAISS.from_documents(chunk, self.embeddings )
        print('Vector store created successfully')


    def retrival(self, query: str, k: int= 5)->List[Document]:
        '''
        Retrieve relavent document for a given query
        Args: 
            query (str): query to search for
            k (int): number of documents to retrieve
        Returns:
            List[Document]: list of retrieved documents
        '''
        if not self.vectorstore:
            raise ValueError('Vector store not initialized..')
        docs = self.vectorstore.similarity_search(query, k)
        # Create context from retrieved documents
        context = "\n\n".join([doc.page_content for doc in docs])
        return context
    
    def generate(self, query: str, context: str, filename:str) -> str:
          
          """Generate response using Groq API
          Args:
                query (str) "Query to search for"
        Returns:
               str: json file exracted from response

        """
          
          prompt = self.prompt.format(context=context, query=query)
          
          chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a specialized AI system for structured data extraction from unstructured text, with particular expertise in menu analysis."},
                    {"role": "user", "content": prompt}
                ],
                model=self.extracter_model_name,
                temperature=0.1,
                max_tokens=4096,
            )
          
          response = chat_completion.choices[0].message.content
          json_data = re.search(r'```json\n(.*?)\n```', response, re.DOTALL).group(1)


          output_path = os.path.join(self.extracted_menu_dir, f'raw_{filename}_menu_extractor.json')
          
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(json_data, f, indent=2, ensure_ascii=False)
          
          with open(output_path, "w") as f:
              f.write(json_data)
          
          print(f"Menu saved successfully to {output_path}")
          
          return output_path
    
    def human_loop_for_llm(self, raw_menu_items: json):
        '''
        Function to review the extracted menu items and allow for manual editing or deletion.
        Args: 
            raw_menu_items (json): json file with extracted menu items
        Returns:
            reviewed_data (json): json file with reviewed and edited menu items
        '''
        print('Review the JSON data before finalizing')

        with open(raw_menu_items , 'r') as f:
            raw_menu_items = json.load(f)

        reviewed_data = []
    
        for indx, category_block in enumerate(raw_menu_items):
            print(f'here the all items of first cat {indx+1} {category_block['category']}')
            print(f"\nðŸ“¦ Category {indx+1}: {category_block['category']}")
            for items in category_block['items']:
                print(f"ðŸŸ¢ Item Name     : {items['item_name']}")
                print(f"ðŸ’° Price         : {items['price']}")
                print(f"ðŸ“ Description   : {items['description'] or 'N/A'}")
                print('---')
        
                action = input("Is this correct? (y/n): or press 'd' to delete or 'all' to save raw json file  âž¤ ").strip().lower()
            
                if action == 'y':
                    print("âœ… Item confirmed.")
                    reviewed_data.append(items)
            
                elif action == 'n':
                    print("âœï¸  Editing the category.")
                    category_block['category'] = input(f"    Edit Category Name [{category_block['category']}]: ") or category_block['category']
                    print("âœï¸  Editing the item.")
                    items['item_name'] = input(f"    Edit Item Name [{items['item_name']}]: ") or items['item_name']
                    items['price'] = input(f"    Edit Price [{items['price']}]: ") or items['price']
                    items['description'] = input(f"    Edit Description [{items['description']}]: ") or items['description']
                    reviewed_data.append(items)
            
                elif action == 'd':
                    print("ðŸ—‘ï¸  Item deleted.")
                    # Do not add to reviewed_data
    
                elif action == 'all':
                    reviewed_data.append(category_block)
                    break
    
                else:
                    print("âš ï¸  Invalid input. Please enter 'y', 'n', or 'd'")
                # Optionally re-prompt here if you want strict control
        
        output_file_path = os.path.join(self.extracted_menu_dir, f"manual_{self.final_json_name}")
        if not reviewed_data:
            print("âŒ No items were confirmed. Exiting...")
            save_json_file(path=output_file_path, data= raw_menu_items)

            return raw_menu_items
        
        save_json_file(path=output_file_path, data= reviewed_data)
        return reviewed_data


    def _organize_text_by_layout(self, ocr_data: Dict[str, Any]) -> List[Dict[str, Any]]:
            """Organize OCR results into layout-aware sections"""
            layout_text = []
            
            # Group by block
            current_block = None
            current_lines = []
            current_line = []
            
            for i in range(len(ocr_data["text"])):
                if ocr_data["text"][i].strip():
                    word_info = {
                        "text": ocr_data["text"][i],
                        "conf": ocr_data["conf"][i],
                        "bbox": [
                            ocr_data["left"][i],
                            ocr_data["top"][i],
                            ocr_data["left"][i] + ocr_data["width"][i],
                            ocr_data["top"][i] + ocr_data["height"][i]
                        ],
                        "block_num": ocr_data["block_num"][i],
                        "line_num": ocr_data["line_num"][i],
                        "par_num": ocr_data["par_num"][i],
                    }
                    
                    if current_block is None or current_block != ocr_data["block_num"][i]:
                        if current_block is not None:
                            if current_line:
                                current_lines.append(current_line)
                            layout_text.append({
                                "block_num": current_block,
                                "lines": current_lines
                            })
                        current_block = ocr_data["block_num"][i]
                        current_lines = []
                        current_line = [word_info]
                    elif current_line and current_line[0]["line_num"] != ocr_data["line_num"][i]:
                        current_lines.append(current_line)
                        current_line = [word_info]
                    else:
                        current_line.append(word_info)
            
            # Add the last block
            if current_line:
                current_lines.append(current_line)
            if current_block is not None:
                layout_text.append({
                    "block_num": current_block,
                    "lines": current_lines
                })
            
            return layout_text
    
    
    def extract_text_from_image(self, image_path: str) -> Dict[str, Any]:
        """
        Extract text from an image using OCR.
        Args:
            image_path (str): Path to the image file.
        Returns:
            str: Extracted text
        
        """
        image = Image.open(image_path).convert("RGB")
        pytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"
    
        # Get OCR results with layout information
        ocr_results = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
        processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=True)
    
        # Process with LayoutLMv3 to get layout-aware features
        encoding = processor(image, return_tensors="pt", truncation=True)
        
        # Organize text by layout
        layout_text = self._organize_text_by_layout(ocr_results)
        
        return {
            "text": pytesseract.image_to_string(image),
            "layout_text": layout_text,
            "image_size": image.size,
            "ocr_data": ocr_results
        }
    
    
    def _create_layout_aware_text(self, extracted_data: Dict[str, Any]) -> str:
            """Convert extracted OCR data into layout-aware text format"""
            layout_text = ""
            
            for block in extracted_data["layout_text"]:
                block_text = ""
                for line in block["lines"]:
                    line_text = " ".join([word["text"] for word in line])
                    block_text += line_text + "\n"
                
                layout_text += f"[BLOCK]\n{block_text}[/BLOCK]\n\n"
            
            return layout_text
    