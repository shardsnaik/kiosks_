{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c818b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\kiosks_\\\\main\\\\components\\\\speech_recognition'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88f019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/kiosks_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a95e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.functions.common_function import read_yaml_file, create_directories\n",
    "from pathlib import Path\n",
    "class speech_recog_config:\n",
    "    def __init__(self):\n",
    "        self.config = read_yaml_file(Path('config/config.yaml'))\n",
    "        self.params = read_yaml_file(Path('config/params.yaml'))\n",
    "\n",
    "    def speech_recog_module(self):\n",
    "        config = self.config.local_models\n",
    "        create_directories([config.speech_recog_model])\n",
    "\n",
    "        sppech_recog_config = {\n",
    "            'model_path': Path(config['speech_recog_model']),\n",
    "            'model': config['speech_to_text_model'],\n",
    "            'duration': self.params['duration'],\n",
    "            'sample_rate': self.params['sampling_rate'],\n",
    "            'channel': self.params['channel'],\n",
    "        }\n",
    "        \n",
    "        return sppech_recog_config\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e51ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "class speech_recog_compo:\n",
    "    def __init__(self, config, parmas):\n",
    "        # obj = speech_recog()\n",
    "        # self.configs = obj.speech_recog_module()\n",
    "        # self.model_path = self.configs['model_path']\n",
    "        #  print(self.configs)\n",
    "        self.config = config\n",
    "        self.params = parmas\n",
    "\n",
    "\n",
    "    def download_model(self):\n",
    "        os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"  # <-- ADD THIS LINE\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        \n",
    "        model_size = self.config['model']\n",
    "        model = WhisperModel(model_size, download_root= self.config['model_path'], device=device, compute_type=\"float32\")\n",
    "        print('model succefully downloaded', model)\n",
    "\n",
    "    \n",
    "    def record_and_transcribe_audio(self):\n",
    "        print('Recording audio...')\n",
    "        audio = sounddevice.rec(int(self.params['duration'] * self.params['sample_rate']), samplerate=self.params['sample_rate'], channels=1, dtype='int16')\n",
    "        sounddevice.wait()\n",
    "        write('recording.wav', self.params['sample_rate'], audio)\n",
    "        print(\"‚úÖ Recording saved to\", \"recording.wav\")\n",
    "        model = WhisperModel(self.config['model'], compute_type=\"float32\")  # use float16 if GPU available\n",
    "\n",
    "        segments, info = model.transcribe(\"recording.wav\", beam_size=5)\n",
    "        print(\"üåê Detected Language:\", info.language)\n",
    "        print(\"üìù Transcription:\")\n",
    "        for segment in segments:\n",
    "            print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e89a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory at: models/speech_recog_model\n"
     ]
    }
   ],
   "source": [
    "obj = speech_recog_config()\n",
    "aa = obj.speech_recog_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a1bf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model succefully downloaded <faster_whisper.transcribe.WhisperModel object at 0x0000026427429310>\n",
      "Recording audio...\n",
      "‚úÖ Recording saved to recording.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\kiosks_\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Systran--faster-whisper-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Detected Language: en\n",
      "üìù Transcription:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = speech_recog_compo(aa, aa)\n",
    "f.download_model()\n",
    "f.record_and_transcribe_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89baa4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d8ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c378881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_audio(filename=\"recording.wav\", duration=5, samplerate=16000):\n",
    "    print(\"üé§ Recording...\")\n",
    "    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    write(filename, samplerate, audio)\n",
    "    print(\"‚úÖ Recording saved to\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording...\n",
      "‚úÖ Recording saved to recording.wav\n",
      "üåê Detected Language: en\n",
      "üìù Transcription:\n",
      "[0.00s -> 4.00s]  Hello, hello, hello\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def transcribe_audio(filename=\"recording.wav\"):\n",
    "    model = WhisperModel(\"base\", compute_type=\"float32\")  # use float16 if GPU available\n",
    "    # model = WhisperModel(\"large-v3\", compute_type=\"float32\")  # use float16 if GPU available\n",
    "\n",
    "    segments, info = model.transcribe(filename, beam_size=5)\n",
    "    print(\"üåê Detected Language:\", info.language)\n",
    "    print(\"üìù Transcription:\")\n",
    "    for segment in segments:\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio(duration=8)  # speak something in any language\n",
    "    transcribe_audio(\"recording.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa77b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dcab0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # new client instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=\"recording_api_whisper.wav\", duration=5, samplerate=16000):\n",
    "    print(\"üé§ Recording...\")\n",
    "    audio = sounddevice.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sounddevice.wait()\n",
    "    write(filename, samplerate, audio)\n",
    "    print(\"‚úÖ Recording saved to\", filename)\n",
    "\n",
    "\n",
    "def transcribe_with_openai(filename=\"recording_api_whisper.wav\"):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\"\n",
    "        )\n",
    "    print(\"üìù Transcription:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b3a1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording...\n",
      "‚úÖ Recording saved to recording_api_whisper.wav\n",
      "üìù Transcription: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    record_audio(duration=5)  # from earlier\n",
    "    transcribe_with_openai(\"recording_api_whisper.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f61f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\kiosks_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/kiosks_')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af502aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.components.speech_recognition.config.speech_recog_config import speech_recog_config\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import speech_recognition as sr\n",
    "from scipy.io.wavfile import write\n",
    "import os, wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory at: models/speech_recog_model\n",
      "model succefully downloaded <faster_whisper.transcribe.WhisperModel object at 0x0000021E7C1B4440>\n",
      "Recording audio...\n",
      "üéôÔ∏è Listening... Speak now (will stop automatically when you're silent)\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "WaitTimeoutError",
     "evalue": "listening timed out while waiting for phrase to start",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWaitTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m f = speech_recog_compo(config_params, config_params)\n\u001b[32m     96\u001b[39m f.download_model()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_and_transcribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mspeech_recog_compo.record_and_transcribe_audio\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# audio = recog.listen(source, \u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# timeout=None, phrase_time_limit=None)\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# self.is_recording = True\u001b[39;00m\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# while self.is_recording:\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRecording...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     audio = \u001b[43mrecog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     all_audio.append(audio.get_wav_data())\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPhrase captured. Continue speaking or press Ctrl+C to finish.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[39m, in \u001b[36mRecognizer.listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    458\u001b[39m result = \u001b[38;5;28mself\u001b[39m._listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\kiosks_\\myenv\\Lib\\site-packages\\speech_recognition\\__init__.py:490\u001b[39m, in \u001b[36mRecognizer._listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    488\u001b[39m elapsed_time += seconds_per_buffer\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time > timeout:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[33m\"\u001b[39m\u001b[33mlistening timed out while waiting for phrase to start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    492\u001b[39m buffer = source.stream.read(source.CHUNK)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n",
      "\u001b[31mWaitTimeoutError\u001b[39m: listening timed out while waiting for phrase to start"
     ]
    }
   ],
   "source": [
    "\n",
    "class speech_recog_compo:\n",
    "    def __init__(self, config, parmas):\n",
    "        # obj = speech_recog()\n",
    "        # self.configs = obj.speech_recog_module()\n",
    "        # self.model_path = self.configs['model_path']\n",
    "        #  print(self.configs)\n",
    "        self.config = config\n",
    "        self.params = parmas\n",
    "        self.is_recording = False\n",
    "\n",
    "\n",
    "\n",
    "    def download_model(self):\n",
    "        os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"  # <-- ADD THIS LINE\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        \n",
    "        model_size = self.config['model']\n",
    "        model = WhisperModel(model_size, download_root= self.config['model_path'], device=device, compute_type=\"float32\")\n",
    "        print('model succefully downloaded', model)\n",
    "\n",
    "    \n",
    "    def record_and_transcribe_audio(self):\n",
    "        print('Recording audio...')\n",
    "\n",
    "        # audio = sounddevice.rec(int(self.params['duration'] * self.params['sample_rate']), samplerate=self.params['sample_rate'], channels=1, dtype='int16')\n",
    "\n",
    "        \n",
    "        # üéôÔ∏è recording usign speech_recognition model\n",
    "        print(\"üéôÔ∏è Listening... Speak now (will stop automatically when you're silent)\")\n",
    "\n",
    "        recog = sr.Recognizer()\n",
    "        mic = sr.Microphone(sample_rate=self.params['sample_rate'])\n",
    "        all_audio =[]\n",
    "     \n",
    "        with mic as source:\n",
    "            recog.adjust_for_ambient_noise(source)\n",
    "        # audio = recog.listen(source, \n",
    "        # timeout=None, phrase_time_limit=None)\n",
    "            # self.is_recording = True\n",
    "\n",
    "            # while self.is_recording:\n",
    "            print('Recording...')\n",
    "            audio = recog.listen(source,timeout=10, phrase_time_limit=None)\n",
    "            all_audio.append(audio.get_wav_data())\n",
    "            print(\"Phrase captured. Continue speaking or press Ctrl+C to finish.\")\n",
    "            \n",
    "        if not all_audio:\n",
    "            print(\"No audio was recorded.\")\n",
    "            return\n",
    "        \n",
    "        # Combine all audio chunks\n",
    "        wav_file = \"recording.wav\"\n",
    "        self._combine_audio_data(all_audio, wav_file)\n",
    "        \n",
    "        print(\"‚úÖ Recording saved to\", wav_file)\n",
    "        \n",
    "        # Transcribe the audio\n",
    "        self._transcribe_audio(wav_file)\n",
    "    \n",
    "    def _combine_audio_data(self, audio_data_chunks, output_filename):\n",
    "        \"\"\"Combine multiple audio chunks into a single WAV file\"\"\"\n",
    "        if not audio_data_chunks:\n",
    "            return\n",
    "        \n",
    "        # Get parameters from the first chunk\n",
    "        with wave.open(audio_data_chunks[0], 'rb') as first_chunk:\n",
    "            params = first_chunk.getparams()\n",
    "        \n",
    "        # Write all chunks to a single file\n",
    "        with wave.open(output_filename, 'wb') as output_file:\n",
    "            output_file.setparams(params)\n",
    "            for chunk in audio_data_chunks:\n",
    "                with wave.open(chunk, 'rb') as chunk_file:\n",
    "                    output_file.writeframes(chunk_file.readframes(chunk_file.getnframes()))\n",
    "    \n",
    "    def _transcribe_audio(self, wav_file):\n",
    "        \"\"\"Transcribe the recorded audio using Whisper\"\"\"\n",
    "        model = WhisperModel(self.config['model'], compute_type=self.config['compute_type'])\n",
    "        \n",
    "        segments, info = model.transcribe(wav_file, log_progress=True, beam_size=5)\n",
    "        print(\"üåê Detected Language:\", info.language)\n",
    "        print(\"üìù Transcription:\")\n",
    "        for segment in segments:\n",
    "            print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "\n",
    "obj = speech_recog_config()\n",
    "config_params = obj.speech_recog_module()\n",
    "\n",
    "f = speech_recog_compo(config_params, config_params)\n",
    "f.download_model()\n",
    "f.record_and_transcribe_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf0fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13e20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "057160b2",
   "metadata": {},
   "source": [
    "<!-- Windows PowerShell\n",
    "Copyright (C) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows\n",
    "\n",
    "PS C:\\kiosks_\\config> cd..\n",
    "PS C:\\kiosks_> myenv/Scripts/activate\n",
    "(myenv) PS C:\\kiosks_> & C:/kiosks_/myenv/Scripts/python.exe c:/kiosks_/main/components/speech_recognition/speech_recog.py\n",
    "Traceback (most recent call last):\n",
    "  File \"c:\\kiosks_\\main\\components\\speech_recognition\\speech_recog.py\", line 1, in <module>\n",
    "    from main.components.speech_recognition.config.speech_recog_config import speech_recog_config\n",
    "ModuleNotFoundError: No module named 'main'\n",
    "(myenv) PS C:\\kiosks_> $PYTHONPATH = 'C:/kiosks_'\n",
    "(myenv) PS C:\\kiosks_> dir\n",
    "\n",
    "\n",
    "    Directory: C:\\kiosks_\n",
    "\n",
    "\n",
    "Mode                 LastWriteTime         Length Name\n",
    "----                 -------------         ------ ----\n",
    "d-----        05-04-2025     17:13                artifacts\n",
    "d-----        12-04-2025     21:47                config\n",
    "d-----        11-04-2025     17:50                main\n",
    "d-----        12-04-2025     19:30                models\n",
    "d-----        07-04-2025     15:07                myenv\n",
    "-a----        05-04-2025     12:13           3617 .gitignore\n",
    "-a----        11-04-2025     15:35           9659 app.py\n",
    "-a----        05-04-2025     12:13          11558 LICENSE\n",
    "-a----        05-04-2025     12:13              9 README.md\n",
    "-a----        12-04-2025     23:52         256044 recording.wav\n",
    "-a----        12-04-2025     21:54            171 requirements.txt\n",
    "\n",
    "\n",
    "(myenv) PS C:\\kiosks_> cd main\n",
    "(myenv) PS C:\\kiosks_\\main> dir\n",
    "\n",
    "\n",
    "    Directory: C:\\kiosks_\\main\n",
    "\n",
    "\n",
    "Mode                 LastWriteTime         Length Name\n",
    "----                 -------------         ------ ----\n",
    "d-----        11-04-2025     16:26                components\n",
    "d-----        09-04-2025     14:36                functions\n",
    "d-----        11-04-2025     17:50                models\n",
    "d-----        11-04-2025     15:35                pipelines\n",
    "d-----        05-04-2025     19:47                predifined\n",
    "\n",
    "\n",
    "(myenv) PS C:\\kiosks_\\main> cd componets\n",
    "cd : Cannot find path 'C:\\kiosks_\\main\\componets' because it does not exist.\n",
    "At line:1 char:1\n",
    "+ cd componets\n",
    "+ ~~~~~~~~~~~~\n",
    "    + CategoryInfo          : ObjectNotFound: (C:\\kiosks_\\main\\componets:String) [Set-Location], ItemNotFoundException\n",
    "    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.SetLocationCommand\n",
    "\n",
    "(myenv) PS C:\\kiosks_\\main> cd components\n",
    "(myenv) PS C:\\kiosks_\\main\\components> dir\n",
    "\n",
    "\n",
    "    Directory: C:\\kiosks_\\main\\components\n",
    "\n",
    "\n",
    "Mode                 LastWriteTime         Length Name\n",
    "----                 -------------         ------ ----\n",
    "d-----        11-04-2025     14:58                menu_maneger\n",
    "d-----        12-04-2025     23:54                speech_recognition\n",
    "\n",
    "\n",
    "(myenv) PS C:\\kiosks_\\main\\components> cd speech_recognition\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> dir\n",
    "\n",
    "\n",
    "    Directory: C:\\kiosks_\\main\\components\\speech_recognition\n",
    "\n",
    "\n",
    "Mode                 LastWriteTime         Length Name\n",
    "----                 -------------         ------ ----\n",
    "d-----        12-04-2025     23:53                config\n",
    "-a----        12-04-2025     23:18         256044 recording.wav\n",
    "-a----        12-04-2025     23:56           1941 speech_recog.py\n",
    "-a----        12-04-2025     23:52           8566 trial.ipynb\n",
    "\n",
    "\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> python speech_recog.py\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\kiosks_\\main\\components\\speech_recognition\\speech_recog.py\", line 1, in <module>\n",
    "    from main.components.speech_recognition.config.speech_recog_config import speech_recog_config\n",
    "ModuleNotFoundError: No module named 'main'\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> $env:PYTHONPATH = 'C:/kiosks_'\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> python speech_recog.py\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\kiosks_\\main\\components\\speech_recognition\\speech_recog.py\", line 45, in <module>\n",
    "    obj = speech_recog_config()\n",
    "          ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\kiosks_\\main\\components\\speech_recognition\\config\\speech_recog_config.py\", line 5, in __init__\n",
    "    self.config = read_yaml_file(Path('config/config.yaml'))\n",
    "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\kiosks_\\main\\functions\\common_function.py\", line 15, in read_yaml_file\n",
    "    with open(path) as file:\n",
    "         ^^^^^^^^^^\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'config\\\\config.yaml'\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> python speech_recog.py\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\kiosks_\\main\\components\\speech_recognition\\speech_recog.py\", line 45, in <module>\n",
    "    obj = speech_recog_config()\n",
    "          ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\kiosks_\\main\\components\\speech_recognition\\config\\speech_recog_config.py\", line 5, in __init__\n",
    "    self.config = read_yaml_file(Path('config/config.yaml'))\n",
    "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\kiosks_\\main\\functions\\common_function.py\", line 15, in read_yaml_file\n",
    "    with open(path) as file:\n",
    "         ^^^^^^^^^^\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'config\\\\config.yaml'\n",
    "(myenv) PS C:\\kiosks_\\main\\components\\speech_recognition> cd..\n",
    "(myenv) PS C:\\kiosks_\\main\\components> cd..\n",
    "(myenv) PS C:\\kiosks_\\main> cd..\n",
    "(myenv) PS C:\\kiosks_> python speech_recog.py\n",
    "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\python.exe: can't open file 'C:\\\\kiosks_\\\\speech_recog.py': [Errno 2] No such file or directory\n",
    "(myenv) PS C:\\kiosks_> & C:/kiosks_/myenv/Scripts/python.exe c:/kiosks_/main/components/speech_recognition/speech_recog.py\n",
    "created directory at: models/speech_recog_model\n",
    "model succefully downloaded <faster_whisper.transcribe.WhisperModel object at 0x0000017D1760B830>\n",
    "Recording audio...\n",
    "‚úÖ Recording saved to recording.wav\n",
    "üåê Detected Language: en\n",
    "üìù Transcription:\n",
    "[0.00s -> 4.00s]  Hello? Hello?\n",
    "[4.00s -> 6.00s]  Hello?\n",
    "(myenv) PS C:\\kiosks_> -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
