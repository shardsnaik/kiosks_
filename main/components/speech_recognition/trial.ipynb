{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c818b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\kiosks_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88f019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/kiosks_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a95e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.functions.common_function import read_yaml_file, create_directories\n",
    "from pathlib import Path\n",
    "class speech_recog_config:\n",
    "    def __init__(self):\n",
    "        self.config = read_yaml_file(Path('config/config.yaml'))\n",
    "        self.params = read_yaml_file(Path('config/params.yaml'))\n",
    "\n",
    "    def speech_recog_module(self):\n",
    "        config = self.config.local_models\n",
    "        create_directories([config.speech_recog_model])\n",
    "\n",
    "        sppech_recog_config = {\n",
    "            'model_path': Path(config['speech_recog_model']),\n",
    "            'model': config['speech_to_text_model'],\n",
    "            'duration': self.params['duration'],\n",
    "            'sample_rate': self.params['sampling_rate'],\n",
    "            'channel': self.params['channel'],\n",
    "        }\n",
    "        \n",
    "        return sppech_recog_config\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e51ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "class speech_recog_compo:\n",
    "    def __init__(self, config, parmas):\n",
    "        # obj = speech_recog()\n",
    "        # self.configs = obj.speech_recog_module()\n",
    "        # self.model_path = self.configs['model_path']\n",
    "        #  print(self.configs)\n",
    "        self.config = config\n",
    "        self.params = parmas\n",
    "\n",
    "\n",
    "    def download_model(self):\n",
    "        os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"  # <-- ADD THIS LINE\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        \n",
    "        model_size = self.config['model']\n",
    "        model = WhisperModel(model_size, download_root= self.config['model_path'], device=device, compute_type=\"float32\")\n",
    "        print('model succefully downloaded', model)\n",
    "\n",
    "    \n",
    "    def record_and_transcribe_audio(self):\n",
    "        print('Recording audio...')\n",
    "        audio = sounddevice.rec(int(self.params['duration'] * self.params['sample_rate']), samplerate=self.params['sample_rate'], channels=1, dtype='int16')\n",
    "        sounddevice.wait()\n",
    "        write('recording.wav', self.params['sample_rate'], audio)\n",
    "        print(\"‚úÖ Recording saved to\", \"recording.wav\")\n",
    "        model = WhisperModel(self.config['model'], compute_type=\"float32\")  # use float16 if GPU available\n",
    "\n",
    "        segments, info = model.transcribe(\"recording.wav\", beam_size=5)\n",
    "        print(\"üåê Detected Language:\", info.language)\n",
    "        print(\"üìù Transcription:\")\n",
    "        for segment in segments:\n",
    "            print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e89a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory at: models/speech_recog_model\n"
     ]
    }
   ],
   "source": [
    "obj = speech_recog_config()\n",
    "aa = obj.speech_recog_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a1bf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model succefully downloaded <faster_whisper.transcribe.WhisperModel object at 0x0000026427429310>\n",
      "Recording audio...\n",
      "‚úÖ Recording saved to recording.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\kiosks_\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Systran--faster-whisper-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Detected Language: en\n",
      "üìù Transcription:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = speech_recog_compo(aa, aa)\n",
    "f.download_model()\n",
    "f.record_and_transcribe_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89baa4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d8ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c378881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def record_audio(filename=\"recording.wav\", duration=5, samplerate=16000):\n",
    "    print(\"üé§ Recording...\")\n",
    "    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    write(filename, samplerate, audio)\n",
    "    print(\"‚úÖ Recording saved to\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording...\n",
      "‚úÖ Recording saved to recording.wav\n",
      "üåê Detected Language: en\n",
      "üìù Transcription:\n",
      "[0.00s -> 4.00s]  Hello, hello, hello\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "def transcribe_audio(filename=\"recording.wav\"):\n",
    "    model = WhisperModel(\"base\", compute_type=\"float32\")  # use float16 if GPU available\n",
    "    # model = WhisperModel(\"large-v3\", compute_type=\"float32\")  # use float16 if GPU available\n",
    "\n",
    "    segments, info = model.transcribe(filename, beam_size=5)\n",
    "    print(\"üåê Detected Language:\", info.language)\n",
    "    print(\"üìù Transcription:\")\n",
    "    for segment in segments:\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_audio(duration=8)  # speak something in any language\n",
    "    transcribe_audio(\"recording.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa77b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dcab0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sounddevice\n",
    "from scipy.io.wavfile import write\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # new client instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=\"recording_api_whisper.wav\", duration=5, samplerate=16000):\n",
    "    print(\"üé§ Recording...\")\n",
    "    audio = sounddevice.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sounddevice.wait()\n",
    "    write(filename, samplerate, audio)\n",
    "    print(\"‚úÖ Recording saved to\", filename)\n",
    "\n",
    "\n",
    "def transcribe_with_openai(filename=\"recording_api_whisper.wav\"):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            response_format=\"text\"\n",
    "        )\n",
    "    print(\"üìù Transcription:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b3a1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Recording...\n",
      "‚úÖ Recording saved to recording_api_whisper.wav\n",
      "üìù Transcription: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    record_audio(duration=5)  # from earlier\n",
    "    transcribe_with_openai(\"recording_api_whisper.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f61f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\kiosks_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/kiosks_')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af502aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.components.speech_recognition.config.speech_recog_config import speech_recog_config\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import speech_recognition as sr\n",
    "from scipy.io.wavfile import write\n",
    "import os, wave, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class speech_recog_compo:\n",
    "    def __init__(self, config, parmas):\n",
    "        # obj = speech_recog()\n",
    "        # self.configs = obj.speech_recog_module()\n",
    "        # self.model_path = self.configs['model_path']\n",
    "        #  print(self.configs)\n",
    "        self.config = config\n",
    "        self.params = parmas\n",
    "        self.is_recording = False\n",
    "\n",
    "\n",
    "\n",
    "    def download_model(self):\n",
    "        os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"  # <-- ADD THIS LINE\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        \n",
    "        model_size = self.config['model']\n",
    "        model = WhisperModel(model_size, download_root= self.config['model_path'], device=device, compute_type=\"float32\")\n",
    "        print('model succefully downloaded', model)\n",
    "\n",
    "    \n",
    "    def record_and_transcribe_audio(self):\n",
    "        print(\"üéôÔ∏è Listening... Speak now. Recording will stop automatically when you're silent.\\n(Press Ctrl+C to stop manually)\\n\")\n",
    "\n",
    "        recog = sr.Recognizer()\n",
    "        mic = sr.Microphone(sample_rate=self.params['sample_rate'])\n",
    "        all_audio = []\n",
    "\n",
    "        # try:\n",
    "        with mic as source:\n",
    "            recog.adjust_for_ambient_noise(source)\n",
    "            while True:\n",
    "                print(\"üéß Listening for next phrase...\")\n",
    "                try:\n",
    "                    audio = recog.listen(source, timeout=2, phrase_time_limit=10)\n",
    "                    all_audio.append(audio.get_wav_data())\n",
    "                    print(\"‚úÖ Phrase captured.\")\n",
    "                except sr.WaitTimeoutxError:\n",
    "                    print(\"‚èπÔ∏è Detected silence. Ending recording.\")\n",
    "                    break\n",
    "        # except KeyboardInterrupt:\n",
    "        #     print(\"\\nüõë Manual stop triggered.\")\n",
    "\n",
    "        if not all_audio:\n",
    "            print(\"‚ö†Ô∏è No audio was recorded.\")\n",
    "            return\n",
    "\n",
    "        wav_file = \"recording.wav\"\n",
    "        self._combine_audio_data(all_audio, wav_file)\n",
    "        print(\"‚úÖ Recording saved to:\", wav_file)\n",
    "\n",
    "        self._transcribe_audio(wav_file)\n",
    "    \n",
    "    def _combine_audio_data(self, audio_data_chunks, output_filename):\n",
    "        \"\"\"Combine multiple audio chunks into a single WAV file\"\"\"\n",
    "        if not audio_data_chunks:\n",
    "            return\n",
    "        \n",
    "        # Get parameters from the first chunk\n",
    "        with wave.open(io.BytesIO(audio_data_chunks[0]), 'rb') as first_chunk:\n",
    "            params = first_chunk.getparams()\n",
    "        \n",
    "        # Write all chunks to a single file\n",
    "        with wave.open(output_filename, 'wb') as output_file:\n",
    "            output_file.setparams(params)\n",
    "\n",
    "            for chunk in audio_data_chunks:\n",
    "                with wave.open(io.BytesIO(chunk), 'rb') as chunk_file:\n",
    "                    output_file.writeframes(chunk_file.readframes(chunk_file.getnframes()))\n",
    "    \n",
    "    def _transcribe_audio(self, wav_file):\n",
    "        \"\"\"Transcribe the recorded audio using Whisper\"\"\"\n",
    "        model = WhisperModel(self.config['model'], compute_type=self.config['compute_type'])\n",
    "        \n",
    "        segments, info = model.transcribe(wav_file, log_progress=True, beam_size=5)\n",
    "        print(\"üåê Detected Language:\", info.language)\n",
    "        print(\"üìù Transcription:\")\n",
    "        for segment in segments:\n",
    "            print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5a80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created directory at: models/speech_recog_model\n",
      "model succefully downloaded <faster_whisper.transcribe.WhisperModel object at 0x000002AB90542720>\n",
      "üéôÔ∏è Listening... Speak now. Recording will stop automatically when you're silent.\n",
      "(Press Ctrl+C to stop manually)\n",
      "\n",
      "üéß Listening for next phrase...\n",
      "‚úÖ Phrase captured.\n",
      "üéß Listening for next phrase...\n",
      "‚úÖ Phrase captured.\n",
      "üéß Listening for next phrase...\n",
      "‚úÖ Phrase captured.\n",
      "üéß Listening for next phrase...\n",
      "‚úÖ Phrase captured.\n",
      "üéß Listening for next phrase...\n",
      "‚úÖ Phrase captured.\n",
      "üéß Listening for next phrase...\n",
      "‚èπÔ∏è Detected silence. Ending recording.\n",
      "‚úÖ Recording saved to: recording.wav\n",
      "üåê Detected Language: en\n",
      "üìù Transcription:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.55/15.55 [00:03<00:00,  3.99seconds/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 6.00s]  I hope this isn't continuously speaking. Until I stop speaking, it will just...\n",
      "[6.00s -> 12.00s]  Until I just stop speaking, it will continue. Until I just keep talking, it will keep listening.\n",
      "[12.00s -> 16.00s]  Just that's it. Now I'm gonna hop. So it will be like...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obj = speech_recog_config()\n",
    "config_params = obj.speech_recog_module()\n",
    "\n",
    "f = speech_recog_compo(config_params, config_params)\n",
    "f.download_model()\n",
    "f.record_and_transcribe_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21c3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
