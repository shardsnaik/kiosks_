{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6f9847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Kiosks_'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('c://Kiosks_')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb540614",
   "metadata": {},
   "source": [
    "## Trying diffrent model for core \n",
    "## huggingface smollm-360m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d298ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8556\\441562301.py:8: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  model.save_pretrained('models\\huggingface_smolLM-360M')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_8556\\441562301.py:9: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  tokenizer.save_pretrained('models\\huggingface_smolLM-360M')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models\\\\huggingface_smolLM-360M\\\\tokenizer_config.json',\n",
       " 'models\\\\huggingface_smolLM-360M\\\\special_tokens_map.json',\n",
       " 'models\\\\huggingface_smolLM-360M\\\\vocab.json',\n",
       " 'models\\\\huggingface_smolLM-360M\\\\merges.txt',\n",
       " 'models\\\\huggingface_smolLM-360M\\\\added_tokens.json',\n",
       " 'models\\\\huggingface_smolLM-360M\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-360M\"\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             ).to(device)\n",
    "model.save_pretrained('models\\huggingface_smolLM-360M')\n",
    "tokenizer.save_pretrained('models\\huggingface_smolLM-360M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78b6fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of india?\n",
      "- 10 What is the capital of India in 2020?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "inputs = tokenizer.encode(\"what is the capital of india\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6e4ac",
   "metadata": {},
   "source": [
    "# Trying GPT-2 Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedf59a",
   "metadata": {},
   "source": [
    "downloading model to desired folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a44536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2Model.from_pretrained('gpt2-medium')\n",
    "\n",
    "tokenizer.save_pretrained('models/Gpt-2-medium')\n",
    "model.save_pretrained('models/Gpt-2-medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1dd275",
   "metadata": {},
   "source": [
    "loading the model from local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b479839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/Gpt-2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models/Gpt-2-medium\")\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('models/Gpt-2-medium')\n",
    "# model = GPT2Model.from_pretrained('models/Gpt-2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7319ad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of india? answer in one word: money.\"\n",
      "\n",
      "[Page 19]\n",
      "\n",
      "The fact is that the English speaking people can and do make decisions and make decisions that do not benefit the working people and the peasantry\n"
     ]
    }
   ],
   "source": [
    "text = \"what is the capital of india? answer in one word\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output_ids = model.generate(**encoded_input, max_length=50, do_sample=True)\n",
    "\n",
    "# Decode output IDs to readable text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65e49e",
   "metadata": {},
   "source": [
    "### Trying Qwen1.5-0.5B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "881e97a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\kiosks_\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-1.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cpu\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # \"Qwen/Qwen1.5-0.5B-Chat\",\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct',\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# prompt = \"what is the capital of india answer in one word\"\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained('models/Qwen2.5-1.5B-Instruct')\n",
    "model.save_pretrained('models/Qwen2.5-1.5B-Instruct')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8ab9c",
   "metadata": {},
   "source": [
    "loading nd generating the Qwen-2.5-0.5b model is around ~7 to 13sec \n",
    "output of Qwen-2.5-1.5b model ~~30sec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19bb22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/Qwen2.5-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4051eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt = \"what is the capital of karnataka and how many district are there in karanataka\"\n",
    "prompt = \"hello may i see the menu\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d601b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Of course, I'd be happy to help you with your inquiry about the menu. What would you like to know or order? Please let me know if there's anything specific that you're interested in, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2038bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as an AI language model, I do not have access to your device or internet connection. However, if you tell me what you're looking for in the menu, I can try to provide some guidance on how to find it.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62868b",
   "metadata": {},
   "source": [
    "### Trying Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3348fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/Qwen2.5-0.5B-v2-finetunned-Instruct-merged\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/Qwen2.5-0.5B-v2-finetunned-Instruct-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe7fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! What would you like to order?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompt = \"what is the capital of karnataka and how many district are there in karanataka\"\n",
    "prompt = \"hello may i see the menu\"\n",
    "device = \"cpu\" # the device to load the model onto\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e32dc",
   "metadata": {},
   "source": [
    "### Trying Qwen2.5 1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a261fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/Qwen2.5-1.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d1b0560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm sorry, but as an AI language model, I don't have access to menus or any external information sources. However, if you let me know what type of restaurant or food item you're interested in, I can try to provide some general information on popular menu items or suggest a local restaurant recommendation based on your preferences. Let me know how I can help!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompt = \"what is the capital of karnataka and how many district are there in karanataka\"\n",
    "prompt = \"hello may i see the menu\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a67b0",
   "metadata": {},
   "source": [
    "### Trying Finetuned Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badb3e4",
   "metadata": {},
   "source": [
    "merge the fine-tuned adapter (LoRA) weights into the base model, then save a merged full-precision model which can run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff68381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\tokenizer_config.json',\n",
       " 'models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\special_tokens_map.json',\n",
       " 'models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\vocab.json',\n",
       " 'models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\merges.txt',\n",
       " 'models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\added_tokens.json',\n",
       " 'models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')\n",
    "base_model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')\n",
    "\n",
    "# loading the fine tunned adaptor base \n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, 'models\\\\Qwen2.5-0.5B-v2-finetuned')\n",
    "\n",
    "# Merge adapter weights into base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged full model\n",
    "model.save_pretrained('models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged')\n",
    "tokenizer.save_pretrained('models\\\\Qwen2.5-0.5B-v2-finetunned-Instruct-merged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd6ce31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19900\\773835285.py:3: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "  model = AutoModelForCausalLM.from_pretrained(\"models\\Qwen2.5-0.5B-finetunned-Instruct-merged\").cpu()\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19900\\773835285.py:4: SyntaxWarning: invalid escape sequence '\\Q'\n",
      "  tokenizer = AutoTokenizer.from_pretrained(\"models\\Qwen2.5-0.5B-finetunned-Instruct-merged\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"models\\Qwen2.5-0.5B-finetunned-Instruct-merged\").cpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models\\Qwen2.5-0.5B-finetunned-Instruct-merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff47c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'hello may i see the menu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d0f8d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "hello may i see the menu\n",
      "assistant\n",
      "Sure! Here's the menu. Feel free to order whenever you're ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompt = \"<|im_start|>user\\nCan I pay with UPI?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = f\"<|im_start|>user\\n{input}\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33eb8265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,    872,    198,  14990,   1231,    600,   1490,    279,   5022,\n",
       "            198, 151644,  77091,    198,  39814,      0,   5692,    594,    279,\n",
       "           5022,     13,  31733,   1910,    311,   1973,  15356,    498,   2299,\n",
       "           5527,     13, 151645]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec4211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
